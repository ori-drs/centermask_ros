{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import numpy as np\n",
    "import atexit\n",
    "import bisect\n",
    "import multiprocessing as mp\n",
    "from collections import deque\n",
    "import cv2\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from detectron2.data import MetadataCatalog\n",
    "from detectron2.engine.defaults import DefaultPredictor\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "\n",
    "\n",
    "class VisualizationDemo(object):\n",
    "    def __init__(self, cfg, instance_mode=ColorMode.IMAGE, parallel=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "            instance_mode (ColorMode):\n",
    "            parallel (bool): whether to run the model in different processes from visualization.\n",
    "                Useful since the visualization logic can be slow.\n",
    "        \"\"\"\n",
    "        self.metadata = MetadataCatalog.get(\n",
    "            cfg.DATASETS.TEST[0] if len(cfg.DATASETS.TEST) else \"__unused\"\n",
    "        )\n",
    "        self.cpu_device = torch.device(\"cpu\")\n",
    "        self.instance_mode = instance_mode\n",
    "\n",
    "        self.parallel = parallel\n",
    "        if parallel:\n",
    "            num_gpu = torch.cuda.device_count()\n",
    "            self.predictor = AsyncPredictor(cfg, num_gpus=num_gpu)\n",
    "        else:\n",
    "            self.predictor = DefaultPredictor(cfg)\n",
    "\n",
    "    def run_on_image(self, image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "                This is the format used by OpenCV.\n",
    "\n",
    "        Returns:\n",
    "            predictions (dict): the output of the model.\n",
    "            vis_output (VisImage): the visualized image output.\n",
    "        \"\"\"\n",
    "        vis_output = None\n",
    "        predictions = self.predictor(image)\n",
    "        # Convert image from OpenCV BGR format to Matplotlib RGB format.\n",
    "        image = image[:, :, ::-1]\n",
    "        visualizer = Visualizer(image, self.metadata, instance_mode=self.instance_mode)\n",
    "        if \"inst\" in predictions:\n",
    "            visualizer.vis_inst(predictions[\"inst\"])\n",
    "        if \"bases\" in predictions:\n",
    "            self.vis_bases(predictions[\"bases\"])\n",
    "        if \"panoptic_seg\" in predictions:\n",
    "            panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
    "            vis_output = visualizer.draw_panoptic_seg_predictions(\n",
    "                panoptic_seg.to(self.cpu_device), segments_info\n",
    "            )\n",
    "        else:\n",
    "            if \"sem_seg\" in predictions:\n",
    "                vis_output = visualizer.draw_sem_seg(\n",
    "                    predictions[\"sem_seg\"].argmax(dim=0).to(self.cpu_device))\n",
    "            if \"instances\" in predictions:\n",
    "                instances = predictions[\"instances\"].to(self.cpu_device)\n",
    "                vis_output = visualizer.draw_instance_predictions(predictions=instances)\n",
    "\n",
    "        return predictions, vis_output\n",
    "\n",
    "    def _frame_from_video(self, video):\n",
    "        while video.isOpened():\n",
    "            success, frame = video.read()\n",
    "            if success:\n",
    "                yield frame\n",
    "            else:\n",
    "                break\n",
    "\n",
    "    def vis_bases(self, bases):\n",
    "        basis_colors = [[2, 200, 255], [107, 220, 255], [30, 200, 255], [60, 220, 255]]\n",
    "        bases = bases[0].squeeze()\n",
    "        bases = (bases / 8).tanh().cpu().numpy()\n",
    "        num_bases = len(bases)\n",
    "        fig, axes = plt.subplots(nrows=num_bases // 2, ncols=2)\n",
    "        for i, basis in enumerate(bases):\n",
    "            basis = (basis + 1) / 2\n",
    "            basis = basis / basis.max()\n",
    "            basis_viz = np.zeros((basis.shape[0], basis.shape[1], 3), dtype=np.uint8)\n",
    "            basis_viz[:, :, 0] = basis_colors[i][0]\n",
    "            basis_viz[:, :, 1] = basis_colors[i][1]\n",
    "            basis_viz[:, :, 2] = np.uint8(basis * 255)\n",
    "            basis_viz = cv2.cvtColor(basis_viz, cv2.COLOR_HSV2RGB)\n",
    "            axes[i // 2][i % 2].imshow(basis_viz)\n",
    "        plt.show()\n",
    "\n",
    "    def run_on_video(self, video):\n",
    "        \"\"\"\n",
    "        Visualizes predictions on frames of the input video.\n",
    "\n",
    "        Args:\n",
    "            video (cv2.VideoCapture): a :class:`VideoCapture` object, whose source can be\n",
    "                either a webcam or a video file.\n",
    "\n",
    "        Yields:\n",
    "            ndarray: BGR visualizations of each video frame.\n",
    "        \"\"\"\n",
    "        video_visualizer = VideoVisualizer(self.metadata, self.instance_mode)\n",
    "\n",
    "        def process_predictions(frame, predictions):\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "            if \"panoptic_seg\" in predictions:\n",
    "                panoptic_seg, segments_info = predictions[\"panoptic_seg\"]\n",
    "                vis_frame = video_visualizer.draw_panoptic_seg_predictions(\n",
    "                    frame, panoptic_seg.to(self.cpu_device), segments_info\n",
    "                )\n",
    "            elif \"instances\" in predictions:\n",
    "                predictions = predictions[\"instances\"].to(self.cpu_device)\n",
    "                vis_frame = video_visualizer.draw_instance_predictions(frame, predictions)\n",
    "            elif \"sem_seg\" in predictions:\n",
    "                vis_frame = video_visualizer.draw_sem_seg(\n",
    "                    frame, predictions[\"sem_seg\"].argmax(dim=0).to(self.cpu_device)\n",
    "                )\n",
    "\n",
    "            # Converts Matplotlib RGB format to OpenCV BGR format\n",
    "            vis_frame = cv2.cvtColor(vis_frame.get_image(), cv2.COLOR_RGB2BGR)\n",
    "            return vis_frame\n",
    "\n",
    "        frame_gen = self._frame_from_video(video)\n",
    "        if self.parallel:\n",
    "            buffer_size = self.predictor.default_buffer_size\n",
    "\n",
    "            frame_data = deque()\n",
    "\n",
    "            for cnt, frame in enumerate(frame_gen):\n",
    "                frame_data.append(frame)\n",
    "                self.predictor.put(frame)\n",
    "\n",
    "                if cnt >= buffer_size:\n",
    "                    frame = frame_data.popleft()\n",
    "                    predictions = self.predictor.get()\n",
    "                    yield process_predictions(frame, predictions)\n",
    "\n",
    "            while len(frame_data):\n",
    "                frame = frame_data.popleft()\n",
    "                predictions = self.predictor.get()\n",
    "                yield process_predictions(frame, predictions)\n",
    "        else:\n",
    "            for frame in frame_gen:\n",
    "                yield process_predictions(frame, self.predictor(frame))\n",
    "\n",
    "\n",
    "class AsyncPredictor:\n",
    "    \"\"\"\n",
    "    A predictor that runs the model asynchronously, possibly on >1 GPUs.\n",
    "    Because rendering the visualization takes considerably amount of time,\n",
    "    this helps improve throughput when rendering videos.\n",
    "    \"\"\"\n",
    "\n",
    "    class _StopToken:\n",
    "        pass\n",
    "\n",
    "    class _PredictWorker(mp.Process):\n",
    "        def __init__(self, cfg, task_queue, result_queue):\n",
    "            self.cfg = cfg\n",
    "            self.task_queue = task_queue\n",
    "            self.result_queue = result_queue\n",
    "            super().__init__()\n",
    "\n",
    "        def run(self):\n",
    "            predictor = DefaultPredictor(self.cfg)\n",
    "\n",
    "            while True:\n",
    "                task = self.task_queue.get()\n",
    "                if isinstance(task, AsyncPredictor._StopToken):\n",
    "                    break\n",
    "                idx, data = task\n",
    "                result = predictor(data)\n",
    "                self.result_queue.put((idx, result))\n",
    "\n",
    "    def __init__(self, cfg, num_gpus: int = 1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "            num_gpus (int): if 0, will run on CPU\n",
    "        \"\"\"\n",
    "        num_workers = max(num_gpus, 1)\n",
    "        self.task_queue = mp.Queue(maxsize=num_workers * 3)\n",
    "        self.result_queue = mp.Queue(maxsize=num_workers * 3)\n",
    "        self.procs = []\n",
    "        for gpuid in range(max(num_gpus, 1)):\n",
    "            cfg = cfg.clone()\n",
    "            cfg.defrost()\n",
    "            cfg.MODEL.DEVICE = \"cuda:{}\".format(gpuid) if num_gpus > 0 else \"cpu\"\n",
    "            self.procs.append(\n",
    "                AsyncPredictor._PredictWorker(cfg, self.task_queue, self.result_queue)\n",
    "            )\n",
    "\n",
    "        self.put_idx = 0\n",
    "        self.get_idx = 0\n",
    "        self.result_rank = []\n",
    "        self.result_data = []\n",
    "\n",
    "        for p in self.procs:\n",
    "            p.start()\n",
    "        atexit.register(self.shutdown)\n",
    "\n",
    "    def put(self, image):\n",
    "        self.put_idx += 1\n",
    "        self.task_queue.put((self.put_idx, image))\n",
    "\n",
    "    def get(self):\n",
    "        self.get_idx += 1  # the index needed for this request\n",
    "        if len(self.result_rank) and self.result_rank[0] == self.get_idx:\n",
    "            res = self.result_data[0]\n",
    "            del self.result_data[0], self.result_rank[0]\n",
    "            return res\n",
    "\n",
    "        while True:\n",
    "            # make sure the results are returned in the correct order\n",
    "            idx, res = self.result_queue.get()\n",
    "            if idx == self.get_idx:\n",
    "                return res\n",
    "            insert = bisect.bisect(self.result_rank, idx)\n",
    "            self.result_rank.insert(insert, idx)\n",
    "            self.result_data.insert(insert, res)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.put_idx - self.get_idx\n",
    "\n",
    "    def __call__(self, image):\n",
    "        self.put(image)\n",
    "        return self.get()\n",
    "\n",
    "    def shutdown(self):\n",
    "        for _ in self.procs:\n",
    "            self.task_queue.put(AsyncPredictor._StopToken())\n",
    "\n",
    "    @property\n",
    "    def default_buffer_size(self):\n",
    "        return len(self.procs) * 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "\n",
    "from detectron2.utils.visualizer import (\n",
    "    ColorMode,\n",
    "    Visualizer,\n",
    "    _create_text_labels,\n",
    "    _PanopticPrediction,\n",
    ")\n",
    "\n",
    "from detectron2.utils.colormap import random_color\n",
    "\n",
    "\n",
    "class _DetectedInstance:\n",
    "    \"\"\"\n",
    "    Used to store data about detected objects in video frame,\n",
    "    in order to transfer color to objects in the future frames.\n",
    "\n",
    "    Attributes:\n",
    "        label (int):\n",
    "        bbox (tuple[float]):\n",
    "        mask_rle (dict):\n",
    "        color (tuple[float]): RGB colors in range (0, 1)\n",
    "        ttl (int): time-to-live for the instance. For example, if ttl=2,\n",
    "            the instance color can be transferred to objects in the next two frames.\n",
    "    \"\"\"\n",
    "\n",
    "    __slots__ = [\"label\", \"bbox\", \"mask_rle\", \"color\", \"ttl\"]\n",
    "\n",
    "    def __init__(self, label, bbox, mask_rle, color, ttl):\n",
    "        self.label = label\n",
    "        self.bbox = bbox\n",
    "        self.mask_rle = mask_rle\n",
    "        self.color = color\n",
    "        self.ttl = ttl\n",
    "\n",
    "\n",
    "class VideoVisualizer:\n",
    "    def __init__(self, metadata, instance_mode=ColorMode.IMAGE):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            metadata (MetadataCatalog): image metadata.\n",
    "        \"\"\"\n",
    "        self.metadata = metadata\n",
    "        self._old_instances = []\n",
    "        assert instance_mode in [\n",
    "            ColorMode.IMAGE,\n",
    "            ColorMode.IMAGE_BW,\n",
    "        ], \"Other mode not supported yet.\"\n",
    "        self._instance_mode = instance_mode\n",
    "\n",
    "    def draw_instance_predictions(self, frame, predictions):\n",
    "        \"\"\"\n",
    "        Draw instance-level prediction results on an image.\n",
    "\n",
    "        Args:\n",
    "            frame (ndarray): an RGB image of shape (H, W, C), in the range [0, 255].\n",
    "            predictions (Instances): the output of an instance detection/segmentation\n",
    "                model. Following fields will be used to draw:\n",
    "                \"pred_boxes\", \"pred_classes\", \"scores\", \"pred_masks\" (or \"pred_masks_rle\").\n",
    "\n",
    "        Returns:\n",
    "            output (VisImage): image object with visualizations.\n",
    "        \"\"\"\n",
    "        frame_visualizer = Visualizer(frame, self.metadata)\n",
    "        num_instances = len(predictions)\n",
    "        if num_instances == 0:\n",
    "            return frame_visualizer.output\n",
    "\n",
    "        boxes = predictions.pred_boxes.tensor.numpy() if predictions.has(\"pred_boxes\") else None\n",
    "        scores = predictions.scores if predictions.has(\"scores\") else None\n",
    "        classes = predictions.pred_classes.numpy() if predictions.has(\"pred_classes\") else None\n",
    "        keypoints = predictions.pred_keypoints if predictions.has(\"pred_keypoints\") else None\n",
    "\n",
    "        if predictions.has(\"pred_masks\"):\n",
    "            masks = predictions.pred_masks\n",
    "            # mask IOU is not yet enabled\n",
    "            # masks_rles = mask_util.encode(np.asarray(masks.permute(1, 2, 0), order=\"F\"))\n",
    "            # assert len(masks_rles) == num_instances\n",
    "        else:\n",
    "            masks = None\n",
    "\n",
    "        detected = [\n",
    "            _DetectedInstance(classes[i], boxes[i], mask_rle=None, color=None, ttl=8)\n",
    "            for i in range(num_instances)\n",
    "        ]\n",
    "        colors = self._assign_colors(detected)\n",
    "\n",
    "        labels = _create_text_labels(classes, scores, self.metadata.get(\"thing_classes\", None))\n",
    "\n",
    "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
    "            # any() returns uint8 tensor\n",
    "            frame_visualizer.output.img = frame_visualizer._create_grayscale_image(\n",
    "                (masks.any(dim=0) > 0).numpy() if masks is not None else None\n",
    "            )\n",
    "            alpha = 0.3\n",
    "        else:\n",
    "            alpha = 0.5\n",
    "\n",
    "        frame_visualizer.overlay_instances(\n",
    "            # boxes=None if masks is not None else boxes,  # boxes are a bit distracting\n",
    "            boxes=boxes,\n",
    "            masks=masks,\n",
    "            labels=labels,\n",
    "            keypoints=keypoints,\n",
    "            assigned_colors=colors,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "\n",
    "        return frame_visualizer.output\n",
    "\n",
    "    def draw_sem_seg(self, frame, sem_seg, area_threshold=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sem_seg (ndarray or Tensor): semantic segmentation of shape (H, W),\n",
    "                each value is the integer label.\n",
    "            area_threshold (Optional[int]): only draw segmentations larger than the threshold\n",
    "        \"\"\"\n",
    "        # don't need to do anything special\n",
    "        frame_visualizer = Visualizer(frame, self.metadata)\n",
    "        frame_visualizer.draw_sem_seg(sem_seg, area_threshold=None)\n",
    "        return frame_visualizer.output\n",
    "\n",
    "    def draw_panoptic_seg_predictions(\n",
    "        self, frame, panoptic_seg, segments_info, area_threshold=None, alpha=0.5\n",
    "    ):\n",
    "        frame_visualizer = Visualizer(frame, self.metadata)\n",
    "        pred = _PanopticPrediction(panoptic_seg, segments_info)\n",
    "\n",
    "        if self._instance_mode == ColorMode.IMAGE_BW:\n",
    "            frame_visualizer.output.img = frame_visualizer._create_grayscale_image(\n",
    "                pred.non_empty_mask()\n",
    "            )\n",
    "\n",
    "        # draw mask for all semantic segments first i.e. \"stuff\"\n",
    "        for mask, sinfo in pred.semantic_masks():\n",
    "            category_idx = sinfo[\"category_id\"]\n",
    "            try:\n",
    "                mask_color = [x / 255 for x in self.metadata.stuff_colors[category_idx]]\n",
    "            except AttributeError:\n",
    "                mask_color = None\n",
    "\n",
    "            frame_visualizer.draw_binary_mask(\n",
    "                mask,\n",
    "                color=mask_color,\n",
    "                text=self.metadata.stuff_classes[category_idx],\n",
    "                alpha=alpha,\n",
    "                area_threshold=area_threshold,\n",
    "            )\n",
    "\n",
    "        all_instances = list(pred.instance_masks())\n",
    "        if len(all_instances) == 0:\n",
    "            return frame_visualizer.output\n",
    "        # draw mask for all instances second\n",
    "        masks, sinfo = list(zip(*all_instances))\n",
    "        num_instances = len(masks)\n",
    "        masks_rles = mask_util.encode(\n",
    "            np.asarray(np.asarray(masks).transpose(1, 2, 0), dtype=np.uint8, order=\"F\")\n",
    "        )\n",
    "        assert len(masks_rles) == num_instances\n",
    "\n",
    "        category_ids = [x[\"category_id\"] for x in sinfo]\n",
    "        detected = [\n",
    "            _DetectedInstance(category_ids[i], bbox=None, mask_rle=masks_rles[i], color=None, ttl=8)\n",
    "            for i in range(num_instances)\n",
    "        ]\n",
    "        colors = self._assign_colors(detected)\n",
    "        labels = [self.metadata.thing_classes[k] for k in category_ids]\n",
    "\n",
    "        frame_visualizer.overlay_instances(\n",
    "            boxes=None,\n",
    "            masks=masks,\n",
    "            labels=labels,\n",
    "            keypoints=None,\n",
    "            assigned_colors=colors,\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        return frame_visualizer.output\n",
    "\n",
    "    def _assign_colors(self, instances):\n",
    "        \"\"\"\n",
    "        Naive tracking heuristics to assign same color to the same instance,\n",
    "        will update the internal state of tracked instances.\n",
    "\n",
    "        Returns:\n",
    "            list[tuple[float]]: list of colors.\n",
    "        \"\"\"\n",
    "\n",
    "        # Compute iou with either boxes or masks:\n",
    "        is_crowd = np.zeros((len(instances),), dtype=np.bool)\n",
    "        if instances[0].bbox is None:\n",
    "            assert instances[0].mask_rle is not None\n",
    "            # use mask iou only when box iou is None\n",
    "            # because box seems good enough\n",
    "            rles_old = [x.mask_rle for x in self._old_instances]\n",
    "            rles_new = [x.mask_rle for x in instances]\n",
    "            ious = mask_util.iou(rles_old, rles_new, is_crowd)\n",
    "            threshold = 0.5\n",
    "        else:\n",
    "            boxes_old = [x.bbox for x in self._old_instances]\n",
    "            boxes_new = [x.bbox for x in instances]\n",
    "            ious = mask_util.iou(boxes_old, boxes_new, is_crowd)\n",
    "            threshold = 0.6\n",
    "        if len(ious) == 0:\n",
    "            ious = np.zeros((len(self._old_instances), len(instances)), dtype=\"float32\")\n",
    "\n",
    "        # Only allow matching instances of the same label:\n",
    "        for old_idx, old in enumerate(self._old_instances):\n",
    "            for new_idx, new in enumerate(instances):\n",
    "                if old.label != new.label:\n",
    "                    ious[old_idx, new_idx] = 0\n",
    "\n",
    "        matched_new_per_old = np.asarray(ious).argmax(axis=1)\n",
    "        max_iou_per_old = np.asarray(ious).max(axis=1)\n",
    "\n",
    "        # Try to find match for each old instance:\n",
    "        extra_instances = []\n",
    "        for idx, inst in enumerate(self._old_instances):\n",
    "            if max_iou_per_old[idx] > threshold:\n",
    "                newidx = matched_new_per_old[idx]\n",
    "                if instances[newidx].color is None:\n",
    "                    instances[newidx].color = inst.color\n",
    "                    continue\n",
    "            # If an old instance does not match any new instances,\n",
    "            # keep it for the next frame in case it is just missed by the detector\n",
    "            inst.ttl -= 1\n",
    "            if inst.ttl > 0:\n",
    "                extra_instances.append(inst)\n",
    "\n",
    "        # Assign random color to newly-detected instances:\n",
    "        for inst in instances:\n",
    "            if inst.color is None:\n",
    "                inst.color = random_color(rgb=True, maximum=1)\n",
    "        self._old_instances = instances[:] + extra_instances\n",
    "        return [d.color for d in instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved\n",
    "import argparse\n",
    "import glob\n",
    "import multiprocessing as mp\n",
    "import os\n",
    "import time\n",
    "import cv2\n",
    "import tqdm\n",
    "import sys\n",
    "\n",
    "from detectron2.data.detection_utils import read_image\n",
    "from detectron2.utils.logger import setup_logger\n",
    "\n",
    "from centermask.config import get_cfg\n",
    "\n",
    "# constants\n",
    "WINDOW_NAME = \"COCO detections\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def setup_cfg(args):\n",
    "    # load config from file and command-line arguments\n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(args.config_file)\n",
    "    cfg.merge_from_list(args.opts)\n",
    "    # Set score_threshold for builtin models\n",
    "    cfg.MODEL.RETINANET.SCORE_THRESH_TEST = args.confidence_threshold\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = args.confidence_threshold\n",
    "    cfg.MODEL.FCOS.INFERENCE_TH_TEST = args.confidence_threshold\n",
    "    cfg.MODEL.PANOPTIC_FPN.COMBINE.INSTANCES_CONFIDENCE_THRESH = args.confidence_threshold\n",
    "    cfg.freeze()\n",
    "    return cfg\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(description=\"Detectron2 Demo\")\n",
    "    parser.add_argument(\n",
    "        \"--config-file\",\n",
    "        default=\"configs/quick_schedules/e2e_mask_rcnn_R_50_FPN_inference_acc_test.yaml\",\n",
    "        metavar=\"FILE\",\n",
    "        help=\"path to config file\",\n",
    "    )\n",
    "    parser.add_argument(\"--webcam\", action=\"store_true\", help=\"Take inputs from webcam.\")\n",
    "    parser.add_argument(\"--video-input\", help=\"Path to video file.\")\n",
    "    parser.add_argument(\"--input\", nargs=\"+\", help=\"A list of space separated input images\")\n",
    "    parser.add_argument(\n",
    "        \"--output\",\n",
    "        help=\"A file or directory to save output visualizations. \"\n",
    "        \"If not given, will show output in an OpenCV window.\",\n",
    "    )\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--confidence-threshold\",\n",
    "        type=float,\n",
    "        default=0.4,\n",
    "        help=\"Minimum score for instance predictions to be shown\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--opts\",\n",
    "        help=\"Modify config options using the command-line 'KEY VALUE' pairs\",\n",
    "        default=[],\n",
    "        nargs=argparse.REMAINDER,\n",
    "    )\n",
    "    return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp.set_start_method(\"spawn\", force=True)\n",
    "# args = get_parser().parse_args(['--config-file', '/root/centermask2/configs/centermask/centermask_lite_V_39_eSE_FPN_ms_4x.yaml'])\n",
    "# args = get_parser().parse_args(['--config-file', '/root/centermask2/configs/centermask/Base-Panoptic-VoVNet-FPN.yaml'])\n",
    "args = get_parser().parse_args(['--config-file', '/root/centermask2/configs/centermask/centermask_R_50_FPN_ms_3x.yaml'])\n",
    "cfg = setup_cfg(args)    \n",
    "logger = setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to do this import so that the backbone registry gets updated \n",
    "import centermask.modeling.backbone\n",
    "demo = VisualizationDemo(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_input = [\"/root/young_tennis_player_186542.jpg\"]\n",
    "args_output = False\n",
    "if args_input:\n",
    "    if os.path.isdir(args_input[0]):\n",
    "        args.input = [os.path.join(args_input[0], fname) for fname in os.listdir(args_input[0])]\n",
    "    elif len(args_input) == 1:\n",
    "        args_input = glob.glob(os.path.expanduser(args_input[0]))\n",
    "        assert args_input, \"The input path(s) was not found\"\n",
    "    for path in tqdm.tqdm(args_input, disable=not args.output):\n",
    "        # use PIL, to be consistent with evaluation\n",
    "        img = read_image(path, format=\"BGR\")\n",
    "        start_time = time.time()\n",
    "        predictions, visualized_output = demo.run_on_image(img)\n",
    "        logger.info(\n",
    "            \"{}: detected {} instances in {:.2f}s\".format(\n",
    "                path, len(predictions[\"instances\"]), time.time() - start_time\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if args_output:\n",
    "            if os.path.isdir(args_output):\n",
    "                assert os.path.isdir(args_output), args.output\n",
    "                out_filename = os.path.join(args_output, os.path.basename(path))\n",
    "            else:\n",
    "                assert len(args_input) == 1, \"Please specify a directory with args.output\"\n",
    "                out_filename = args_output\n",
    "            visualized_output.save(out_filename)\n",
    "        else:\n",
    "            cv2.imshow(WINDOW_NAME, visualized_output.get_image()[:, :, ::-1])\n",
    "            if cv2.waitKey(0) == 27:\n",
    "                break  # esc to quit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
